{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ito0890/DataEnginner-Notebook/blob/main/13_02_Distributedspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c717a0d-cf97-47b1-8430-df3e6e31ef9d"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2a0129-4964-49e3-b38b-cb4cff2980ce"
      },
      "source": [
        "![](http://spark.apache.org/images/spark-logo.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e15ef2de-5d49-49e2-a9d1-f8dbbfa16c70"
      },
      "source": [
        "# Leveraging Apache Spark for Efficient Retail Data Processing at RetailWorld\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82f84108-c3c5-4ac7-ae4c-26d747101757"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "331fae3f-c474-4994-b341-737f9e2ce5a5"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        " - Understand the Distributed Architecture of Spark in the context of a Real Time Problem\n",
        " - Perform Data Parsing and Cleaning of Data\n",
        " - Perform various aggregations to derive insights from the cleaned data\n",
        " - Save the aggregated results to HDFS (Hadoop Distributed File System) for further storage and processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc942532-753f-4230-b889-809f232f2e66"
      },
      "source": [
        "## Background\n",
        "RetailWorld, a prominent retail chain with numerous stores across Metropolia, faces the challenge of processing and analyzing substantial volumes of daily sales data. With real-time data streaming from multiple sources, RetailWorld needs to clean, transform, and aggregate this data to derive actionable insights such as total Sales and Revenue per Product, Total Sales and Revenue per Store, Sales and Revenue per Promotion Type and Stock Analysis per Product.\n",
        "\n",
        "This dataset is a modified  <a href=\"https://www.kaggle.com/datasets/berkayalan/retail-sales-data?select=sales.csv\">sales</a> dataset taken from th Kaggle website. This data is collected from a Turkish retail company, covering the period from the beginning of 2017 to the end of 2019.\n",
        "It currently consists of 1033435 records.\n",
        "\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "\n",
        "**product_id**: This attribute represents the unique identifier for each product in the dataset. Each product is assigned a specific ID (e.g., P0001).\n",
        "\n",
        "**store_id**: This attribute represents the unique identifier for each store where the product is sold. Each store is assigned a specific ID (e.g., S0002).\n",
        "\n",
        "**date**: This attribute represents the date of sales data. It indicates when the sales, revenue, stock, and other information were recorded for a particular product in a specific store.\n",
        "\n",
        "**sales**: This attribute represents the number of units of the product sold on a given date in a particular store. It indicates the quantity of the product that was purchased.\n",
        "\n",
        "**revenue**: This attribute represents the total revenue generated from the sales of the product on a given date in a specific store. It is calculated by multiplying the number of units sold (sales) by the price per unit (price).\n",
        "\n",
        "**stock**: This attribute represents the quantity of the product available in stock at the beginning of the day on the specified date in the given store.\n",
        "\n",
        "**price**: This attribute represents the price per unit of the product on a given date in a specific store. It indicates the amount charged to the customer for each unit of the product.\n",
        "\n",
        "**promo_type_1**: This attribute represents the type of promotion (if any) applied to the product. It indicates the first type of promotional activity associated with the product, such as discounts, special offers, or marketing campaigns.\n",
        "\n",
        "**promo_bin_1**: This attribute represents the specific promotional bin (if any) associated with the first type of promotion. It provides additional details about the nature or category of the promotion.\n",
        "\n",
        "**promo_type_2**: This attribute represents the type of secondary promotion (if any) applied to the product. It indicates another type of promotional activity associated with the product, similar to promo_type_1 but potentially different in nature or timing.\n",
        "\n",
        "These attributes collectively provide detailed information about the sales, revenue, pricing, and promotional activities associated with each product in various stores over time.\n",
        "\n",
        "\n",
        "## Challenges\n",
        "Traditional data processing tools are inadequate for handling the velocity and volume of incoming sales data, leading to delays in analysis and decision-making. These delays hinder RetailWorld's ability to respond swiftly to market demands and optimize inventory and sales strategies.\n",
        "\n",
        "## Solution: Apache Spark\n",
        "To address these challenges, RetailWorld requires a scalable and efficient solution. Apache Spark, with its distributed computing architecture and robust processing capabilities, is the ideal solution for RetailWorld's data analytics needs. Spark's ability to parallelize data processing tasks across a cluster of nodes enables rapid aggregation and analysis of large datasets. Additionally, its fault-tolerant design ensures reliability and resilience against failures, making it a dependable choice for RetailWorld's critical data processing tasks.\n",
        "\n",
        "**To know more about the Distributed Architecture of Spark <a  href=\"https://cf-courses-data.static.labs.skills.network/BmowmNpC6oPodsERIBziYg/DistributedArchApacheSpark-v1.md.html\">Click here</a>**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2015560-91dc-468c-bdc5-3abf35cdfc75"
      },
      "source": [
        "### 1. Install and import the necessary spark libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1155f62a-8f95-4909-bf09-e9d13879d932",
        "outputId": "306c9b25-feb6-4917-fad9-1557c770a8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-spark-connect 0.5.2 requires pyspark>=3.5, but you have pyspark 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install PySpark version 3.1.2 silently\n",
        "!pip install pyspark==3.1.2 -q\n",
        "# Install findSpark silently\n",
        "!pip install findspark -q"
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b01b1560-c59b-4937-96f7-f1f7bdae7d62"
      },
      "outputs": [],
      "source": [
        "# Suppressing warnings by defining a function 'warn' that does nothing\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "# Importing the 'warnings' module to handle warnings\n",
        "import warnings\n",
        "\n",
        "# Overriding the 'warn' function in the 'warnings' module with the defined function to suppress warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "# Filtering out all warnings to be ignored\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# FindSpark simplifies the process of using Apache Spark with Python\n",
        "# Importing the 'findspark' module\n",
        "import findspark\n",
        "\n",
        "# Initializing FindSpark to locate Spark installation\n",
        "findspark.init()\n",
        "\n",
        "# Importing SparkSession from pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n"
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af1f8af0-2fc1-40f5-891e-c5b94077570e"
      },
      "source": [
        "### 2. Initializing the SparkContext\n",
        "\n",
        "The Driver Program initializes the Spark Context and sets the name of the Spark application to **\"RetailStoreSalesAnalysis\"**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "db41567f-c292-4a90-b0ad-8ca07f179be1",
        "outputId": "375c5021-7bd0-4384-a06a-0db2de61c469"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /usr/local/lib/python3.11/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2978)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2925)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2805)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1365)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1337)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:484)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:454)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:427)\n\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:95)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:252)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:189)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:458)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.nio.file.NoSuchFileException: /usr/local/lib/python3.11/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1420)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:841)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:867)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:856)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:258)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:187)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:348)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2900)\n\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:2994)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2952)\n\t... 25 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-178c5f2451b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RetailStoreSalesAnalysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mExamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mpyFiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonExec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYSPARK_PYTHON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"python3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonVer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%d.%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Broadcast's __reduce__ method stores Broadcast instances here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /usr/local/lib/python3.11/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2978)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2925)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2805)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1365)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1337)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendSparkHadoopConfigs(SparkHadoopUtil.scala:484)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.org$apache$spark$deploy$SparkHadoopUtil$$appendS3AndSparkHadoopHiveConfigurations(SparkHadoopUtil.scala:454)\n\tat org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:427)\n\tat org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:122)\n\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:95)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:252)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:189)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:458)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.nio.file.NoSuchFileException: /usr/local/lib/python3.11/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1420)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:841)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:867)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:856)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:258)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:187)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:348)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2900)\n\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:2994)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2952)\n\t... 25 more\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Spark context\n",
        "sc = SparkContext(appName=\"RetailStoreSalesAnalysis\")"
      ],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de49fff9-c70f-44a3-bc92-b83187986ca6",
        "outputId": "370daddb-6b31-4ef1-e1e7-fd4086b55190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-06 17:36:08--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XXlNzqYcxqkTbllc-tL_0w/Retailsales.csv\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47593992 (45M) [text/csv]\n",
            "Saving to: ‘Retailsales.csv.1’\n",
            "\n",
            "Retailsales.csv.1   100%[===================>]  45.39M  18.8MB/s    in 2.4s    \n",
            "\n",
            "2025-04-06 17:36:11 (18.8 MB/s) - ‘Retailsales.csv.1’ saved [47593992/47593992]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XXlNzqYcxqkTbllc-tL_0w/Retailsales.csv"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed714255-6fb2-4200-87dc-8609dbd0f9cb"
      },
      "source": [
        "### 3. Loading Data:\n",
        "It starts by loading data from a CSV file named \"\"Retailsales.csv\" using SparkContext's textFile function. The data is loaded as an **RDD (Resilient Distributed Dataset)** named `raw_data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fda983f-8e36-4ff5-8f21-6fa07b5c0fab"
      },
      "outputs": [],
      "source": [
        "raw_data = sc.textFile(\"Retailsales.csv\")"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4977f5ad-e3ca-4e5a-8118-a68ad1241cf6"
      },
      "source": [
        "### 4 Parsing and Cleaning Data:\n",
        "\n",
        "The `parse_line`function is defined to parse each line of the CSV file into a structured format, extracting fields like **product ID, store ID, date, sales, revenue,** etc.\n",
        "The header line is removed from the RDD.\n",
        "The parsed data is filtered to remove records with missing or invalid data, such as zero or negative sales or price.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "44679265-7a06-402b-b964-f083bf9a1191",
        "outputId": "86d4f5f9-f9f7-40df-f553-d8bd4fd07104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/serializers.py\", line 437, in dumps\n",
            "    return cloudpickle.dumps(obj, pickle_protocol)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 630, in reducer_override\n",
            "    return self._function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 503, in _function_reduce\n",
            "    return self._dynamic_function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
            "    state = _function_getstate(func)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 156, in _function_getstate\n",
            "    f_globals_ref = _extract_code_globals(func.__code__)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                 ~~~~~^^^^^^^\n",
            "IndexError: tuple index out of range\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not serialize object: IndexError: tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mreducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mnewargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_getnewargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_function_getstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         return (types.FunctionType, newargs, state, None, None,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mf_globals_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_code_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3b542d3a0e83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Remove the header line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mraw_data_no_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2949\u001b[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0m\u001b[1;32m   2950\u001b[0m                                       self._jrdd_deserializer, profiler)\n\u001b[1;32m   2951\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2830\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
          ]
        }
      ],
      "source": [
        "# Parse and Clean Data\n",
        "def parse_line(line):\n",
        "    # Split the line by comma to get fields\n",
        "    fields = line.split(\",\")\n",
        "    # Return a dictionary with parsed fields\n",
        "    return {\n",
        "        'product_id': fields[0],\n",
        "        'store_id': fields[1],\n",
        "        'date': fields[2],\n",
        "        'sales': float(fields[3]),\n",
        "        'revenue': float(fields[4]),\n",
        "        'stock': float(fields[5]),\n",
        "        'price': float(fields[6]),\n",
        "        'promo_type_1': fields[7],\n",
        "        'promo_type_2': fields[9]\n",
        "    }\n",
        "\n",
        "# Remove the header line\n",
        "header = raw_data.first()\n",
        "\n",
        "raw_data_no_header = raw_data.filter(lambda line: line != header)\n",
        "\n",
        "# Parse the lines into a structured format\n",
        "parsed_data = raw_data_no_header.map(parse_line)\n",
        "parsed_data = parsed_data.filter(lambda x: x is not None)\n",
        "\n",
        "\n",
        "# Filter out records with missing or invalid data\n",
        "cleaned_data = parsed_data.filter(lambda x: x['sales'] > 0 and x['price'] > 0)"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3859f42c-5bc7-440a-bd34-97ff690f0a64"
      },
      "source": [
        "### 5. Partitioning:\n",
        "\n",
        "The number of partitions in the cleaned data RDD is checked and printed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de70a883-2a54-45a5-9184-e6b8a5cde9e6"
      },
      "outputs": [],
      "source": [
        "# Check the number of partitions\n",
        "print(f\"Number of partitions in cleaned_data: {cleaned_data.getNumPartitions()}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7d99dcd-adf2-4545-8a6a-2343d702aa82"
      },
      "source": [
        "### 6. Partition-wise Count:\n",
        "\n",
        "Here a function `count_in_partition` is defined to count the number of records in each partition of the RDD.\n",
        "This function is applied using `mapPartitionsWithIndex` to get the count of records in each partition, and the results are printed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "554d00ad-0ea6-472f-84f7-eca8ce770ee8"
      },
      "outputs": [],
      "source": [
        "# Function to count the number of records in each partition\n",
        "def count_in_partition(index, iterator):\n",
        "    count = sum(1 for _ in iterator)\n",
        "    yield (index, count)\n",
        "\n",
        "# Get the count of records in each partition\n",
        "partitions_info = cleaned_data.mapPartitionsWithIndex(count_in_partition).collect()\n",
        "print(\"Number of records in each partition:\")\n",
        "for partition, count in partitions_info:\n",
        "    print(f\"Partition {partition}: {count} records\")\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ee84813-0887-410d-9d83-2cae4a5078d0"
      },
      "source": [
        "### 7.Aggregations:\n",
        "\n",
        "Several aggregations are performed on the cleaned data RDD:\n",
        "- Total sales and revenue per product.\n",
        "- Total sales and revenue per store.\n",
        "- Average price per product.\n",
        "- Sales and revenue per promotion type 1 and promotion type 2.\n",
        "- Stock analysis per product.\n",
        "- Each aggregation is performed using map to transform the data into key-value pairs and reduceByKey to aggregate the values for each key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a752173e-068c-469d-b862-a69a379c2dcf"
      },
      "source": [
        "#### a. Total Sales and Revenue per Product:\n",
        "This aggregation calculates the total sales and revenue for each product.\n",
        "It first maps each record in cleaned_data to a key-value pair, where the key is the product ID and the value is a tuple containing sales and revenue.\n",
        "Then, it uses reduceByKey to aggregate the sales and revenue values for each product ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d056cfe1-fa1a-4ea4-a3ad-ad3c7470c33a"
      },
      "outputs": [],
      "source": [
        "# Aggregation 1: Total Sales and Revenue per Product\n",
        "sales_revenue_per_product = cleaned_data.map(lambda x: (x['product_id'], (x['sales'], x['revenue']))) \\\n",
        "                                        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "print(f\"Number of partitions in cleaned_data: {cleaned_data.getNumPartitions()}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "134d26e1-2ad4-45e9-babe-d8822db157b7"
      },
      "source": [
        "#### b. Total Sales and Revenue per Store:\n",
        "This aggregation calculates the total sales and revenue for each store.\n",
        "Similar to the first aggregation, it maps each record to a key-value pair with the store ID as the key and a tuple containing sales and revenue as the value.\n",
        "It then uses reduceByKey to aggregate the sales and revenue values for each store ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3e4c3a1-493d-4334-a2ef-f4b182b1adaf"
      },
      "outputs": [],
      "source": [
        "# Aggregation 2: Total Sales and Revenue per Store\n",
        "sales_revenue_per_store = cleaned_data.map(lambda x: (x['store_id'], (x['sales'], x['revenue']))) \\\n",
        "                                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ba1280-b4aa-406a-b4cc-0292c6c9dedf"
      },
      "source": [
        "#### c.Average Price per Product:\n",
        "This aggregation calculates the average price for each product.\n",
        "It first maps each record to a key-value pair with the product ID as the key and a tuple containing the price and a count of 1.\n",
        "Then, it uses reduceByKey to aggregate the total price and count of prices for each product.\n",
        "Finally, it calculates the average price by dividing the total price by the count.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be246fa9-38ab-4ead-ad59-d04ed0994d48"
      },
      "outputs": [],
      "source": [
        "# Aggregation 3: Average Price per Product\n",
        "total_price_count_per_product = cleaned_data.map(lambda x: (x['product_id'], (x['price'], 1))) \\\n",
        "                                            .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "average_price_per_product = total_price_count_per_product.mapValues(lambda x: x[0] / x[1])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "048c6e54-34c6-4bf5-bfd4-dbb6809ec645"
      },
      "source": [
        "#### d. Sales and Revenue per Promotion Type:\n",
        "These aggregations calculate the total sales and revenue for each promotion type (promo_type_1 and promo_type_2).\n",
        "Each record is mapped to a key-value pair with the promotion type as the key and a tuple containing sales and revenue as the value.\n",
        "Then, reduceByKey is used to aggregate the sales and revenue values for each promotion type.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8985bc25-c148-48c0-95a1-8d0bc2f39d38"
      },
      "outputs": [],
      "source": [
        "# Aggregation 4: Sales and Revenue per Promotion Type\n",
        "sales_revenue_per_promo_1 = cleaned_data.map(lambda x: (x['promo_type_1'], (x['sales'], x['revenue']))) \\\n",
        "                                        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "sales_revenue_per_promo_2 = cleaned_data.map(lambda x: (x['promo_type_2'], (x['sales'], x['revenue']))) \\\n",
        "                                        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2f9a32f-4872-4cf6-9ac7-c3de8170a46c"
      },
      "source": [
        "### 8. Stock Analysis per Product:\n",
        "This aggregation calculates the total stock for each product.\n",
        "Each record is mapped to a key-value pair with the product ID as the key and the stock as the value.\n",
        "Then, reduceByKey is used to aggregate the stock values for each product.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdeb7089-c56a-4d07-91b0-4a85fc27b893"
      },
      "outputs": [],
      "source": [
        "# Aggregation 5: Stock Analysis per Product\n",
        "stock_per_product = cleaned_data.map(lambda x: (x['product_id'], x['stock'])) \\\n",
        "                                .reduceByKey(lambda a, b: a + b)\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "416b8fd4-3914-4341-83e7-b082c1dcc7b3"
      },
      "source": [
        "### 9. Saving Results:\n",
        "\n",
        "The results of each aggregation are saved to HDFS (Hadoop Distributed File System) using saveAsTextFile.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b07ddb9d-5047-4ec1-bda4-070a89baf5e1"
      },
      "outputs": [],
      "source": [
        "# Save results to HDFS\n",
        "sales_revenue_per_product.saveAsTextFile(\"sales_revenue_per_product\")\n",
        "sales_revenue_per_store.saveAsTextFile(\"sales_revenue_per_store\")\n",
        "average_price_per_product.saveAsTextFile(\"average_price_per_product\")\n",
        "sales_revenue_per_promo_1.saveAsTextFile(\"sales_revenue_per_promo_1\")\n",
        "sales_revenue_per_promo_2.saveAsTextFile(\"sales_revenue_per_promo_2\")\n",
        "stock_per_product.saveAsTextFile(\"stock_per_product\")\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "507b829a-df72-43ab-8260-cee373a98fdf"
      },
      "source": [
        "### 10. Printing Results:\n",
        "\n",
        "Finally, we prints the results of each aggregation by collecting the data from the RDDs and iterating over them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eaaf244-ad62-4f25-af25-ced5448ce24e"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "print(\"Total Sales and Revenue per Product:\")\n",
        "print(\"=\" * 35)\n",
        "for product in sales_revenue_per_product.collect():\n",
        "    # Create the format string with appropriate padding\n",
        "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
        "\n",
        "    # Print the values using the format string\n",
        "    print(format_string.format(str(product[0]), str(round(product[1][0],2)), str(round(product[1][1],2))))\n",
        "\n",
        "print(\"\\n\\nTotal Sales and Revenue per Store:\")\n",
        "print(\"=\" * 35)\n",
        "for store in sales_revenue_per_store.collect():\n",
        "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
        "    print(format_string.format(str(store[0]), str(round(store[1][0],2)), str(round(store[1][1],2))))\n",
        "\n",
        "print(\"\\n\\nAverage Price per Product:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "for product in average_price_per_product.collect():\n",
        "    format_string = f\"{{:<5}} | {{:<9}}\"\n",
        "    print(format_string.format(str(product[0]), str(round(product[1],2))))\n",
        "\n",
        "print(\"\\n\\nSales and Revenue per Promotion Type 1:\")\n",
        "print(\"=\" * 40)\n",
        "for promo in sales_revenue_per_promo_1.collect():\n",
        "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
        "    print(format_string.format(str(promo[0]), str(round(promo[1][0],2)), str(round(promo[1][1],2))))\n",
        "\n",
        "print(\"\\n\\nSales and Revenue per Promotion Type 2:\")\n",
        "print(\"=\" * 40)\n",
        "for promo in sales_revenue_per_promo_2.collect():\n",
        "    format_string = f\"{{:<5}} | {{:<9}} | {{:<9}}\"\n",
        "\n",
        "    print(format_string.format(str(promo[0]), str(round(promo[1][0],2)), str(round(promo[1][1],2))))\n",
        "\n",
        "print(\"\\n\\nStock per Product:\")\n",
        "print(\"=\" * 20)\n",
        "for product in stock_per_product.collect():\n",
        "    format_string = f\"{{:<5}} | {{:<9}}\"\n",
        "    print(format_string.format(str(product[0]), str(round(product[1],2))))\n",
        "\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3498ec15-6ea4-4c9f-b088-cedc44bab597"
      },
      "source": [
        "### 11.Cleanup:\n",
        "\n",
        "The Spark context is stopped to release the resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be166920-8790-4b6e-913c-8d8f357331fa"
      },
      "outputs": [],
      "source": [
        "# Stop the Spark context\n",
        "sc.stop()\n"
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7f9d2a-4f95-45e5-97a8-3ccbfcb882e9"
      },
      "source": [
        "## Author(s)\n",
        "\n",
        "Lakshmi Holla\n",
        "\n",
        "## Other Contributors\n",
        "Malika Singla\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "88d818beea56e3725e531236e9db7678e45a99a432baaad8c37d2d9ea80626b2",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}